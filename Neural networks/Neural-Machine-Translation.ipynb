{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation (NMT)\n",
    "Objective:\n",
    "- Build a Neural Machine Translation (NMT) model to translate human readable dates (\"25th of June, 2009\") into machine readable dates (\"2009-06-25\")\n",
    "\n",
    "\n",
    "Exemple:\n",
    "\n",
    "`9 may 1998 -> 1998-05-09\n",
    "10.11.19 -> 2019-11-10\n",
    "9/10/70 -> 1970-09-10\n",
    "saturday april 28 1990 -> 1990-04-28\n",
    "thursday january 26 1995 -> 1995-01-26\n",
    "monday march 7 1983 -> 1983-03-07`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow.keras.utils as ku\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/Haller-x/Data-Science/main/data/dates_dataset.csv'\n",
    "data = pd.read_csv(url_data, header=None)\n",
    "train_index = int(0.9 * len(data))\n",
    "train_data = data[:train_index]\n",
    "test_data = data[train_index:]\n",
    "\n",
    "token_origin = tokenenizer(char_level=True)\n",
    "token_end = tokenenizer(char_level = True)\n",
    "\n",
    "token_origin.fit_on_texts(train_data.loc[:,0])\n",
    "token_end.fit_on_texts(train_data.loc[:,1])\n",
    "\n",
    "seq_train = token_origin.texts_to_sequences(train_data.loc[:,0])\n",
    "seq_test = token_origin.texts_to_sequences(test_data.loc[:,0])\n",
    "\n",
    "label_train = np.array(token_end.texts_to_sequences(train_data.loc[:,1]))\n",
    "label_test = np.array(token_end.texts_to_sequences(test_data.loc[:,1]))\n",
    "\n",
    "label_train = ku.to_categorical(label_train, num_classes = len(token_end.word_index)+1)\n",
    "label_test = ku.to_categorical(label_test, num_classes = len(token_end.word_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size_orig = 30\n",
    "max_size_dest = 10\n",
    "\n",
    "padding_type = 'post'\n",
    "truncating_type = 'post'\n",
    "\n",
    "padded_train = pad_sequences(seq_train, maxlen = max_size_orig, padding=padding_type, truncating=truncating_type)\n",
    "padded_test = pad_sequences(seq_test, maxlen = max_size_orig, padding=padding_type, truncating=truncating_type) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 50)            1800      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 10, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 50)            20200     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 10, 12)            612       \n",
      "=================================================================\n",
      "Total params: 42,812\n",
      "Trainable params: 42,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Reshape, Lambda, Dropout, TimeDistributed, RepeatVector\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(token_origin.word_index)+1, 50,  input_length=max_size_orig))\n",
    "model.add(LSTM(50))\n",
    "model.add(RepeatVector(10))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(token_end.word_index)+1, activation='softmax')))\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,restore_best_weights=True)\n",
    "\n",
    "board = tf.keras.callbacks.TensorBoard(log_dir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.138799). Check your callbacks.\n",
      "282/282 - 4s - loss: 1.8568 - accuracy: 0.3448 - val_loss: 1.3719 - val_accuracy: 0.4842\n",
      "Epoch 2/60\n",
      "282/282 - 3s - loss: 1.2314 - accuracy: 0.4810 - val_loss: 1.1912 - val_accuracy: 0.4927\n",
      "Epoch 3/60\n",
      "282/282 - 3s - loss: 1.1844 - accuracy: 0.4853 - val_loss: 1.1798 - val_accuracy: 0.4910\n",
      "Epoch 4/60\n",
      "282/282 - 3s - loss: 1.1761 - accuracy: 0.4854 - val_loss: 1.1684 - val_accuracy: 0.4960\n",
      "Epoch 5/60\n",
      "282/282 - 3s - loss: 1.0290 - accuracy: 0.5777 - val_loss: 0.9586 - val_accuracy: 0.6048\n",
      "Epoch 6/60\n",
      "282/282 - 4s - loss: 0.8973 - accuracy: 0.6269 - val_loss: 0.8480 - val_accuracy: 0.6457\n",
      "Epoch 7/60\n",
      "282/282 - 3s - loss: 0.8005 - accuracy: 0.6632 - val_loss: 0.7604 - val_accuracy: 0.6842\n",
      "Epoch 8/60\n",
      "282/282 - 2s - loss: 0.7237 - accuracy: 0.7001 - val_loss: 0.6989 - val_accuracy: 0.7096\n",
      "Epoch 9/60\n",
      "282/282 - 3s - loss: 0.6553 - accuracy: 0.7330 - val_loss: 0.6246 - val_accuracy: 0.7342\n",
      "Epoch 10/60\n",
      "282/282 - 2s - loss: 0.5756 - accuracy: 0.7600 - val_loss: 0.5483 - val_accuracy: 0.7723\n",
      "Epoch 11/60\n",
      "282/282 - 3s - loss: 0.5138 - accuracy: 0.7871 - val_loss: 0.4733 - val_accuracy: 0.8055\n",
      "Epoch 12/60\n",
      "282/282 - 3s - loss: 0.4377 - accuracy: 0.8246 - val_loss: 0.4219 - val_accuracy: 0.8289\n",
      "Epoch 13/60\n",
      "282/282 - 3s - loss: 0.3747 - accuracy: 0.8533 - val_loss: 0.3538 - val_accuracy: 0.8672\n",
      "Epoch 14/60\n",
      "282/282 - 3s - loss: 0.3251 - accuracy: 0.8804 - val_loss: 0.3127 - val_accuracy: 0.8833\n",
      "Epoch 15/60\n",
      "282/282 - 2s - loss: 0.2801 - accuracy: 0.9061 - val_loss: 0.2610 - val_accuracy: 0.9125\n",
      "Epoch 16/60\n",
      "282/282 - 3s - loss: 0.2365 - accuracy: 0.9276 - val_loss: 0.2414 - val_accuracy: 0.9136\n",
      "Epoch 17/60\n",
      "282/282 - 2s - loss: 0.2002 - accuracy: 0.9402 - val_loss: 0.1923 - val_accuracy: 0.9426\n",
      "Epoch 18/60\n",
      "282/282 - 2s - loss: 0.1671 - accuracy: 0.9524 - val_loss: 0.1574 - val_accuracy: 0.9544\n",
      "Epoch 19/60\n",
      "282/282 - 3s - loss: 0.1397 - accuracy: 0.9628 - val_loss: 0.1407 - val_accuracy: 0.9557\n",
      "Epoch 20/60\n",
      "282/282 - 3s - loss: 0.1141 - accuracy: 0.9736 - val_loss: 0.1103 - val_accuracy: 0.9725\n",
      "Epoch 21/60\n",
      "282/282 - 3s - loss: 0.0967 - accuracy: 0.9780 - val_loss: 0.0888 - val_accuracy: 0.9786\n",
      "Epoch 22/60\n",
      "282/282 - 3s - loss: 0.0800 - accuracy: 0.9838 - val_loss: 0.0747 - val_accuracy: 0.9850\n",
      "Epoch 23/60\n",
      "282/282 - 3s - loss: 0.0669 - accuracy: 0.9879 - val_loss: 0.0666 - val_accuracy: 0.9881\n",
      "Epoch 24/60\n",
      "282/282 - 4s - loss: 0.0558 - accuracy: 0.9906 - val_loss: 0.0584 - val_accuracy: 0.9882\n",
      "Epoch 25/60\n",
      "282/282 - 4s - loss: 0.0488 - accuracy: 0.9915 - val_loss: 0.0552 - val_accuracy: 0.9883\n",
      "Epoch 26/60\n",
      "282/282 - 3s - loss: 0.0418 - accuracy: 0.9927 - val_loss: 0.0388 - val_accuracy: 0.9935\n",
      "Epoch 27/60\n",
      "282/282 - 4s - loss: 0.0317 - accuracy: 0.9959 - val_loss: 0.0400 - val_accuracy: 0.9908\n",
      "Epoch 28/60\n",
      "282/282 - 4s - loss: 0.0276 - accuracy: 0.9961 - val_loss: 0.0292 - val_accuracy: 0.9946\n",
      "Epoch 29/60\n",
      "282/282 - 3s - loss: 0.0263 - accuracy: 0.9954 - val_loss: 0.0260 - val_accuracy: 0.9951\n",
      "Epoch 30/60\n",
      "282/282 - 4s - loss: 0.0209 - accuracy: 0.9970 - val_loss: 0.0218 - val_accuracy: 0.9962\n",
      "Epoch 31/60\n",
      "282/282 - 2s - loss: 0.0220 - accuracy: 0.9960 - val_loss: 0.0213 - val_accuracy: 0.9960\n",
      "Epoch 32/60\n",
      "282/282 - 2s - loss: 0.0163 - accuracy: 0.9975 - val_loss: 0.0232 - val_accuracy: 0.9956\n",
      "Epoch 33/60\n",
      "282/282 - 2s - loss: 0.0153 - accuracy: 0.9975 - val_loss: 0.0178 - val_accuracy: 0.9965\n",
      "Epoch 34/60\n",
      "282/282 - 1s - loss: 0.0244 - accuracy: 0.9943 - val_loss: 0.0748 - val_accuracy: 0.9727\n",
      "Epoch 35/60\n",
      "282/282 - 2s - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.0127 - val_accuracy: 0.9975\n",
      "Epoch 36/60\n",
      "282/282 - 2s - loss: 0.0070 - accuracy: 0.9996 - val_loss: 0.0134 - val_accuracy: 0.9974\n",
      "Epoch 37/60\n",
      "282/282 - 2s - loss: 0.0120 - accuracy: 0.9977 - val_loss: 0.0123 - val_accuracy: 0.9976\n",
      "Epoch 38/60\n",
      "282/282 - 2s - loss: 0.0063 - accuracy: 0.9994 - val_loss: 0.0091 - val_accuracy: 0.9979\n",
      "Epoch 39/60\n",
      "282/282 - 2s - loss: 0.0114 - accuracy: 0.9976 - val_loss: 0.0171 - val_accuracy: 0.9949\n",
      "Epoch 40/60\n",
      "282/282 - 3s - loss: 0.0154 - accuracy: 0.9959 - val_loss: 0.0157 - val_accuracy: 0.9958\n",
      "Epoch 41/60\n",
      "282/282 - 2s - loss: 0.0066 - accuracy: 0.9990 - val_loss: 0.0081 - val_accuracy: 0.9981\n",
      "Epoch 42/60\n",
      "282/282 - 3s - loss: 0.0050 - accuracy: 0.9993 - val_loss: 0.0064 - val_accuracy: 0.9989\n",
      "Epoch 43/60\n",
      "282/282 - 2s - loss: 0.0028 - accuracy: 0.9999 - val_loss: 0.0068 - val_accuracy: 0.9989\n",
      "Epoch 44/60\n",
      "282/282 - 3s - loss: 0.0184 - accuracy: 0.9945 - val_loss: 0.0395 - val_accuracy: 0.9865\n",
      "Epoch 45/60\n",
      "282/282 - 3s - loss: 0.0099 - accuracy: 0.9976 - val_loss: 0.0061 - val_accuracy: 0.9988\n",
      "Epoch 46/60\n",
      "282/282 - 4s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 0.9990\n",
      "Epoch 47/60\n",
      "282/282 - 4s - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.0041 - val_accuracy: 0.9991\n",
      "Epoch 48/60\n",
      "282/282 - 4s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 0.9993\n",
      "Epoch 49/60\n",
      "282/282 - 3s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
      "Epoch 50/60\n",
      "282/282 - 3s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 0.9994\n",
      "Epoch 51/60\n",
      "282/282 - 3s - loss: 9.5907e-04 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 0.9994\n",
      "Epoch 52/60\n",
      "282/282 - 3s - loss: 8.6697e-04 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
      "Epoch 53/60\n",
      "282/282 - 2s - loss: 7.9456e-04 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 0.9994\n",
      "Epoch 54/60\n",
      "282/282 - 2s - loss: 7.1016e-04 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 0.9994\n",
      "Epoch 55/60\n",
      "282/282 - 2s - loss: 0.0492 - accuracy: 0.9855 - val_loss: 0.0072 - val_accuracy: 0.9982\n",
      "Epoch 56/60\n",
      "282/282 - 2s - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.0034 - val_accuracy: 0.9994\n",
      "Epoch 57/60\n",
      "282/282 - 2s - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.0046 - val_accuracy: 0.9987\n",
      "Epoch 58/60\n",
      "282/282 - 3s - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0070 - val_accuracy: 0.9979\n",
      "Epoch 59/60\n",
      "282/282 - 2s - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0085 - val_accuracy: 0.9977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9d8c1a6290>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "\n",
    "model.fit(padded_train,label_train, epochs = num_epochs, validation_data=(padded_test,label_test), verbose=0 ,\n",
    "         callbacks=[monitor,board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 0.9994\n",
      "Loss: 0.002300913678482175\n",
      "Acc: 0.999\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(padded_test,label_test)\n",
    "print('Loss:',loss)\n",
    "print('Acc:',round(acc,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 9 7 5 - 0 8 - 2 6']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = token_origin.texts_to_sequences(['august 26 1975'])\n",
    "seq\n",
    "padded_seq = pad_sequences(seq, maxlen = max_size_orig, padding=padding_type, truncating=truncating_type)\n",
    "\n",
    "token_end.sequences_to_texts(np.argmax(model.predict(padded_seq), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1975-08-26', '1991-07-12', '1975-09-10']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_dates(dates):\n",
    "    seq = token_origin.texts_to_sequences(dates)\n",
    "    padded_seq = pad_sequences(seq, maxlen = max_size_orig, padding=padding_type, truncating=truncating_type)\n",
    "    converted = token_end.sequences_to_texts(np.argmax(model.predict(padded_seq), axis=-1))\n",
    "    return [i.replace(' ','') for i in converted]\n",
    "                           \n",
    "predict_dates(['august 26 1975', 'friday july 12 1991', '10 sep 1975'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
